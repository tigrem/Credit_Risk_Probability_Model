{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff559b51-94cb-4402-a404-12a953d995a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'D:\\10academy\\week5\\Credit_Risk_Probability_Model' to sys.path.\n",
      "Error importing feature_engineering.py: No module named 'sklearn'\n",
      "Please ensure 'src/feature_engineering.py' exists and is correctly defined.\n",
      "Also, check that the project root (containing 'src') is added to sys.path.\n",
      "\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import display # For better display of dataframes\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "# This allows us to import modules directly from 'src'\n",
    "current_dir = os.getcwd()\n",
    "# Assuming the notebook is in 'your_project/notebooks/'\n",
    "# and src is in 'your_project/src/'\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Added '{project_root}' to sys.path.\")\n",
    "\n",
    "# Import the feature engineering pipeline and its components\n",
    "try:\n",
    "    from src.feature_engineering import (\n",
    "        create_feature_engineering_pipeline,\n",
    "        FeatureExtractor,\n",
    "        RFMCalculator,\n",
    "        AggregateFeatures,\n",
    "        MissingValueHandler,\n",
    "        CustomEncoder,\n",
    "        FeatureScaler\n",
    "    )\n",
    "    print(\"Successfully imported feature_engineering.py components.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing feature_engineering.py: {e}\")\n",
    "    print(\"Please ensure 'src/feature_engineering.py' exists and is correctly defined.\")\n",
    "    print(\"Also, check that the project root (containing 'src') is added to sys.path.\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nSetup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576b5cde-b77a-46a7-8986-c5e01acfe18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Raw Data ---\n",
      "Data loaded successfully from '../data/raw/data.csv'\n",
      "Raw data shape: (95662, 16)\n",
      "\n",
      "Raw data head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionId</th>\n",
       "      <th>BatchId</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>SubscriptionId</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CurrencyCode</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>ProviderId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>ChannelId</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Value</th>\n",
       "      <th>TransactionStartTime</th>\n",
       "      <th>PricingStrategy</th>\n",
       "      <th>FraudResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionId_76871</td>\n",
       "      <td>BatchId_36123</td>\n",
       "      <td>AccountId_3957</td>\n",
       "      <td>SubscriptionId_887</td>\n",
       "      <td>CustomerId_4406</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_6</td>\n",
       "      <td>ProductId_10</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>2018-11-15T02:18:49Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TransactionId_73770</td>\n",
       "      <td>BatchId_15642</td>\n",
       "      <td>AccountId_4841</td>\n",
       "      <td>SubscriptionId_3829</td>\n",
       "      <td>CustomerId_4406</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_4</td>\n",
       "      <td>ProductId_6</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>ChannelId_2</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2018-11-15T02:19:08Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TransactionId_26203</td>\n",
       "      <td>BatchId_53941</td>\n",
       "      <td>AccountId_4229</td>\n",
       "      <td>SubscriptionId_222</td>\n",
       "      <td>CustomerId_4683</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_6</td>\n",
       "      <td>ProductId_1</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>2018-11-15T02:44:21Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TransactionId_380</td>\n",
       "      <td>BatchId_102363</td>\n",
       "      <td>AccountId_648</td>\n",
       "      <td>SubscriptionId_2185</td>\n",
       "      <td>CustomerId_988</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_1</td>\n",
       "      <td>ProductId_21</td>\n",
       "      <td>utility_bill</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>21800</td>\n",
       "      <td>2018-11-15T03:32:55Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TransactionId_28195</td>\n",
       "      <td>BatchId_38780</td>\n",
       "      <td>AccountId_4841</td>\n",
       "      <td>SubscriptionId_3829</td>\n",
       "      <td>CustomerId_988</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_4</td>\n",
       "      <td>ProductId_6</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>ChannelId_2</td>\n",
       "      <td>-644.0</td>\n",
       "      <td>644</td>\n",
       "      <td>2018-11-15T03:34:21Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TransactionId         BatchId       AccountId       SubscriptionId  \\\n",
       "0  TransactionId_76871   BatchId_36123  AccountId_3957   SubscriptionId_887   \n",
       "1  TransactionId_73770   BatchId_15642  AccountId_4841  SubscriptionId_3829   \n",
       "2  TransactionId_26203   BatchId_53941  AccountId_4229   SubscriptionId_222   \n",
       "3    TransactionId_380  BatchId_102363   AccountId_648  SubscriptionId_2185   \n",
       "4  TransactionId_28195   BatchId_38780  AccountId_4841  SubscriptionId_3829   \n",
       "\n",
       "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
       "0  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10   \n",
       "1  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6   \n",
       "2  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1   \n",
       "3   CustomerId_988          UGX          256  ProviderId_1  ProductId_21   \n",
       "4   CustomerId_988          UGX          256  ProviderId_4   ProductId_6   \n",
       "\n",
       "      ProductCategory    ChannelId   Amount  Value  TransactionStartTime  \\\n",
       "0             airtime  ChannelId_3   1000.0   1000  2018-11-15T02:18:49Z   \n",
       "1  financial_services  ChannelId_2    -20.0     20  2018-11-15T02:19:08Z   \n",
       "2             airtime  ChannelId_3    500.0    500  2018-11-15T02:44:21Z   \n",
       "3        utility_bill  ChannelId_3  20000.0  21800  2018-11-15T03:32:55Z   \n",
       "4  financial_services  ChannelId_2   -644.0    644  2018-11-15T03:34:21Z   \n",
       "\n",
       "   PricingStrategy  FraudResult  \n",
       "0                2            0  \n",
       "1                2            0  \n",
       "2                2            0  \n",
       "3                2            0  \n",
       "4                2            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95662 entries, 0 to 95661\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   TransactionId         95662 non-null  object \n",
      " 1   BatchId               95662 non-null  object \n",
      " 2   AccountId             95662 non-null  object \n",
      " 3   SubscriptionId        95662 non-null  object \n",
      " 4   CustomerId            95662 non-null  object \n",
      " 5   CurrencyCode          95662 non-null  object \n",
      " 6   CountryCode           95662 non-null  int64  \n",
      " 7   ProviderId            95662 non-null  object \n",
      " 8   ProductId             95662 non-null  object \n",
      " 9   ProductCategory       95662 non-null  object \n",
      " 10  ChannelId             95662 non-null  object \n",
      " 11  Amount                95662 non-null  float64\n",
      " 12  Value                 95662 non-null  int64  \n",
      " 13  TransactionStartTime  95662 non-null  object \n",
      " 14  PricingStrategy       95662 non-null  int64  \n",
      " 15  FraudResult           95662 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(11)\n",
      "memory usage: 11.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH = '../data/raw/data.csv' # Path to your raw data file\n",
    "\n",
    "# --- Load Data ---\n",
    "print(\"--- Loading Raw Data ---\")\n",
    "try:\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Data loaded successfully from '{DATA_PATH}'\")\n",
    "    print(f\"Raw data shape: {df_raw.shape}\")\n",
    "    print(\"\\nRaw data head:\")\n",
    "    display(df_raw.head())\n",
    "    print(\"\\nRaw data info:\")\n",
    "    df_raw.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {DATA_PATH} not found.\")\n",
    "    print(\"Please ensure 'data.csv' is located in the '../data/raw/' directory relative to this notebook.\")\n",
    "    # Create a dummy dataframe for testing if the file is truly missing\n",
    "    print(\"\\n--- Creating a dummy DataFrame for testing. ---\")\n",
    "    df_raw = pd.DataFrame({\n",
    "        'TransactionId': range(10),\n",
    "        'BatchId': range(100,110),\n",
    "        'AccountId': [1,1,2,2,3,3,1,4,4,5],\n",
    "        'SubscriptionId': range(200,210),\n",
    "        'CustomerId': [10,10,20,20,30,30,10,40,40,50],\n",
    "        'CurrencyCode': ['KES']*10,\n",
    "        'CountryCode': [254]*10,\n",
    "        'ProviderId': [1,2,1,3,2,1,2,3,1,2],\n",
    "        'ProductId': ['P1','P2','P1','P3','P2','P1','P3','P1','P2','P3'],\n",
    "        'ProductCategory': ['CatA','CatB','CatA','CatC','CatB','CatA','CatC','CatA','CatB','CatC'],\n",
    "        'ChannelId': ['App','Web','App','POS','Web','App','POS','Web','App','POS'],\n",
    "        'Amount': [100.5, -50.0, 200.0, 75.2, -30.0, 150.0, 80.0, 120.0, -20.0, 90.0],\n",
    "        'Value': [100.5, 50.0, 200.0, 75.2, 30.0, 150.0, 80.0, 120.0, 20.0, 90.0],\n",
    "        'TransactionStartTime': pd.to_datetime([\n",
    "            '2024-01-01 10:00:00', '2024-01-01 11:30:00', '2024-01-02 09:00:00',\n",
    "            '2024-01-02 14:00:00', '2024-01-03 16:00:00', '2024-01-03 10:00:00',\n",
    "            '2024-01-04 08:00:00', '2024-01-04 12:00:00', '2024-01-05 09:00:00',\n",
    "            '2024-01-05 13:00:00'\n",
    "        ]),\n",
    "        'PricingStrategy': [1]*10,\n",
    "        'FraudResult': [0,0,0,0,1,0,0,0,0,0] # Example fraud result\n",
    "    })\n",
    "    print(f\"Dummy data shape: {df_raw.shape}\")\n",
    "    display(df_raw.head())\n",
    "\n",
    "# Create a working copy for individual tests\n",
    "df_working_copy = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1cf02a-f31b-4440-b88b-73a234be2348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing FeatureExtractor ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'FeatureExtractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 3: Test FeatureExtractor\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Testing FeatureExtractor ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m extractor = \u001b[43mFeatureExtractor\u001b[49m()\n\u001b[32m      5\u001b[39m df_extracted = extractor.fit_transform(df_working_copy.copy()) \u001b[38;5;66;03m# Use a fresh copy for this test\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAfter FeatureExtractor (first 5 rows):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'FeatureExtractor' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test FeatureExtractor\n",
    "\n",
    "print(\"\\n--- Testing FeatureExtractor ---\")\n",
    "extractor = FeatureExtractor()\n",
    "df_extracted = extractor.fit_transform(df_working_copy.copy()) # Use a fresh copy for this test\n",
    "print(\"After FeatureExtractor (first 5 rows):\")\n",
    "display(df_extracted.head())\n",
    "print(f\"New time-based columns: {[col for col in df_extracted.columns if 'Transaction' in col and col != 'TransactionId']}\")\n",
    "print(f\"Original 'TransactionStartTime' column removed: {'TransactionStartTime' not in df_extracted.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19434c3-88d8-4b86-b32c-2b7ae45a2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Test RFMCalculator\n",
    "\n",
    "print(\"\\n--- Testing RFMCalculator ---\")\n",
    "rfm_calc = RFMCalculator()\n",
    "df_rfm = rfm_calc.fit_transform(df_working_copy.copy()) # Use a fresh copy for this test (needs original timestamp)\n",
    "print(\"After RFMCalculator (first 5 rows):\")\n",
    "display(df_rfm.head())\n",
    "print(f\"RFM columns added: {[col for col in ['Recency', 'Frequency', 'Monetary'] if col in df_rfm.columns]}\")\n",
    "print(\"\\nRFM features summary statistics:\")\n",
    "display(df_rfm[['Recency', 'Frequency', 'Monetary']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c996f86-3a4c-4b76-9a29-3a04c150a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Test AggregateFeatures\n",
    "\n",
    "print(\"\\n--- Testing AggregateFeatures ---\")\n",
    "# Ensure 'Amount' is numeric before aggregation\n",
    "df_working_copy['Amount'] = pd.to_numeric(df_working_copy['Amount'], errors='coerce')\n",
    "aggregator = AggregateFeatures()\n",
    "df_aggregated = aggregator.fit_transform(df_working_copy.copy()) # Use a fresh copy for this test\n",
    "print(\"After AggregateFeatures (first 5 rows):\")\n",
    "display(df_aggregated.head())\n",
    "print(f\"Aggregate columns added: {[col for col in df_aggregated.columns if 'TransactionAmount' in col or 'TransactionCount' in col]}\")\n",
    "print(\"\\nAggregate features summary statistics:\")\n",
    "display(df_aggregated[['TotalTransactionAmount', 'AverageTransactionAmount', 'TransactionCount']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49a92d-b872-4d10-a691-db2a4635f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test MissingValueHandler\n",
    "\n",
    "print(\"\\n--- Testing MissingValueHandler ---\")\n",
    "# Create a copy and introduce some NaNs for this specific test\n",
    "df_with_nans = df_working_copy.copy()\n",
    "# Introduce NaN in 'Amount' and 'Value' for a couple of rows\n",
    "df_with_nans.loc[[1, 5], ['Amount', 'Value']] = np.nan\n",
    "# Introduce NaN in a categorical column for testing\n",
    "df_with_nans.loc[[2, 6], 'ProductCategory'] = np.nan\n",
    "df_with_nans.loc[[3], 'ChannelId'] = np.nan\n",
    "\n",
    "print(\"Data before MissingValueHandler (showing NaNs counts):\")\n",
    "print(df_with_nans[['Amount', 'Value', 'ProductCategory', 'ChannelId']].isnull().sum())\n",
    "print(\"\\nBefore imputation - Sample rows with NaNs:\")\n",
    "display(df_with_nans[df_with_nans['Amount'].isnull() | df_with_nans['ProductCategory'].isnull()].head())\n",
    "\n",
    "imputer = MissingValueHandler(strategy='mean')\n",
    "df_imputed = imputer.fit_transform(df_with_nans.copy()) # Use a fresh copy for this test\n",
    "\n",
    "print(\"\\nData after MissingValueHandler (showing NaNs counts):\")\n",
    "print(df_imputed[['Amount', 'Value', 'ProductCategory', 'ChannelId']].isnull().sum())\n",
    "print(\"\\nAfter imputation - Sample rows (should be filled):\")\n",
    "display(df_imputed[df_imputed['TransactionId'].isin([df_with_nans.loc[1, 'TransactionId'], df_with_nans.loc[2, 'TransactionId']])])\n",
    "\n",
    "print(\"\\nValue counts for ProductCategory after imputation (should show no NaNs, most frequent used):\")\n",
    "print(df_imputed['ProductCategory'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa90bc-091f-47b9-b2f8-e6cf89519a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Test CustomEncoder (One-Hot Encoding)\n",
    "\n",
    "print(\"\\n--- Testing CustomEncoder (One-Hot Encoding) ---\")\n",
    "# Start with a dataframe that has no NaNs (e.g., df_imputed from previous step or a fresh copy)\n",
    "df_for_encoding = df_imputed.copy() if 'df_imputed' in locals() else df_working_copy.copy()\n",
    "\n",
    "# Select some categorical columns for encoding\n",
    "categorical_cols_onehot = ['ProductCategory', 'ChannelId', 'CurrencyCode']\n",
    "# Ensure these columns exist and are of object type before encoding\n",
    "cols_to_encode_existing = [col for col in categorical_cols_onehot if col in df_for_encoding.columns and df_for_encoding[col].dtype == 'object']\n",
    "\n",
    "encoder_onehot = CustomEncoder(method='onehot', columns=cols_to_encode_existing)\n",
    "df_encoded_onehot = encoder_onehot.fit_transform(df_for_encoding.copy()) # Use a fresh copy for this test\n",
    "\n",
    "print(f\"Original categorical columns being encoded: {cols_to_encode_existing}\")\n",
    "print(f\"Shape before encoding: {df_for_encoding.shape}\")\n",
    "print(f\"Shape after One-Hot Encoding: {df_encoded_onehot.shape}\")\n",
    "print(\"\\nFirst 5 rows with new one-hot encoded columns:\")\n",
    "display(df_encoded_onehot.head())\n",
    "\n",
    "print(\"\\nPresence of original and new columns:\")\n",
    "for col in cols_to_encode_existing:\n",
    "    print(f\"Is original '{col}' column present? {col in df_encoded_onehot.columns}\")\n",
    "    # Check for some of the new one-hot encoded columns\n",
    "    if col in df_encoded_onehot.columns: # If original column still exists (shouldn't for proper one-hot)\n",
    "        pass\n",
    "    else: # If original column was dropped, check for new dummy variables\n",
    "        example_dummy_cols = [c for c in df_encoded_onehot.columns if c.startswith(f\"{col}_\")][:3]\n",
    "        if example_dummy_cols:\n",
    "            print(f\"Example new dummy columns for '{col}': {example_dummy_cols}\")\n",
    "        else:\n",
    "            print(f\"No new dummy columns found for '{col}'. Check if it was encoded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8d494-c0f8-4c3e-a96c-11ad3b6581ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Test CustomEncoder (Label Encoding)\n",
    "\n",
    "print(\"\\n--- Testing CustomEncoder (Label Encoding) ---\")\n",
    "# Use a fresh copy of the raw data for a clear demonstration of label encoding\n",
    "df_for_label_encoding = df_raw.copy()\n",
    "df_for_label_encoding['ProductCategory'] = df_for_label_encoding['ProductCategory'].fillna('Unknown') # Handle NaNs for label encoding\n",
    "\n",
    "categorical_cols_label = ['ProductId', 'ProductCategory'] # Example columns for label encoding\n",
    "\n",
    "encoder_label = CustomEncoder(method='label', columns=categorical_cols_label)\n",
    "df_encoded_label = encoder_label.fit_transform(df_for_label_encoding.copy()) # Use a fresh copy for this test\n",
    "\n",
    "print(f\"Original categorical columns being encoded: {categorical_cols_label}\")\n",
    "print(\"\\nFirst 5 rows with label encoded columns:\")\n",
    "display(df_encoded_label[categorical_cols_label].head())\n",
    "\n",
    "print(\"\\nUnique encoded values for 'ProductId':\")\n",
    "print(df_encoded_label['ProductId'].unique())\n",
    "print(\"\\nUnique encoded values for 'ProductCategory':\")\n",
    "print(df_encoded_label['ProductCategory'].unique())\n",
    "print(f\"Original 'ProductId' dtype: {df_for_label_encoding['ProductId'].dtype}, Encoded 'ProductId' dtype: {df_encoded_label['ProductId'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8cd6e-49e1-4d40-842c-2320e1f0e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Test FeatureScaler (StandardScaler)\n",
    "\n",
    "print(\"\\n--- Testing FeatureScaler (StandardScaler) ---\")\n",
    "# For this test, let's combine some features from previous steps to ensure we have variety\n",
    "# Start with a dataframe that has numerical columns (e.g., after extraction, RFM, aggregation, imputation)\n",
    "df_for_scaling = df_encoded_onehot.copy() # Using the one-hot encoded and imputed data\n",
    "\n",
    "# Ensure numerical columns are present and identified correctly\n",
    "numerical_cols_to_scale = df_for_scaling.select_dtypes(include=np.number).columns.tolist()\n",
    "# Exclude identifier columns that shouldn't be scaled (if they are numeric)\n",
    "ids_to_exclude = ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'CountryCode', 'PricingStrategy', 'FraudResult', 'ProviderId']\n",
    "numerical_cols_to_scale = [col for col in numerical_cols_to_scale if col not in ids_to_exclude]\n",
    "\n",
    "print(f\"Numerical columns selected for scaling: {numerical_cols_to_scale}\")\n",
    "\n",
    "if not numerical_cols_to_scale:\n",
    "    print(\"No suitable numerical columns found for scaling after previous steps. Skipping scaler test.\")\n",
    "else:\n",
    "    scaler = FeatureScaler(method='standard', columns=numerical_cols_to_scale)\n",
    "    df_scaled = scaler.fit_transform(df_for_scaling.copy()) # Use a fresh copy for this test\n",
    "\n",
    "    print(\"\\nAfter FeatureScaler (first 5 rows of scaled numerical columns):\")\n",
    "    display(df_scaled[numerical_cols_to_scale].head())\n",
    "\n",
    "    print(\"\\nMean and Standard Deviation of scaled columns (should be ~0 and ~1):\")\n",
    "    display(df_scaled[numerical_cols_to_scale].describe().loc[['mean', 'std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49eadd2-9461-465a-9fff-a0225ca2aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Test Full Pipeline\n",
    "\n",
    "print(\"\\n--- Testing Full Feature Engineering Pipeline ---\")\n",
    "\n",
    "# Initialize the full pipeline\n",
    "# You can customize imputation strategy and encoding method here\n",
    "full_pipeline = create_feature_engineering_pipeline(\n",
    "    numerical_imputation_strategy='mean',\n",
    "    categorical_encoding_method='onehot'\n",
    ")\n",
    "\n",
    "# Apply the full pipeline to the raw data\n",
    "print(f\"Shape of raw data: {df_raw.shape}\")\n",
    "df_processed_full = full_pipeline.fit_transform(df_raw.copy()) # Use a fresh copy of the raw data\n",
    "\n",
    "print(\"\\n--- Full Processed Data Overview ---\")\n",
    "print(f\"Shape of fully processed data: {df_processed_full.shape}\")\n",
    "print(\"\\nFirst 5 rows of fully processed data:\")\n",
    "display(df_processed_full.head())\n",
    "print(\"\\nInformation about fully processed columns:\")\n",
    "df_processed_full.info()\n",
    "\n",
    "print(\"\\n--- Final Check for Missing Values in Fully Processed Data ---\")\n",
    "final_missing_values = df_processed_full.isnull().sum()\n",
    "final_missing_values = final_missing_values[final_missing_values > 0]\n",
    "print(final_missing_values)\n",
    "if final_missing_values.empty:\n",
    "    print(\"No missing values found in the final processed data.\")\n",
    "else:\n",
    "    print(\"Warning: Missing values still present after full pipeline. Investigate columns listed above.\")\n",
    "\n",
    "print(\"\\n--- Feature Engineering Pipeline Testing Completed ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

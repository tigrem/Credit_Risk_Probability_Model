{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc5895a-b160-44f4-ae72-5260ac12db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Added '{project_root}' to sys.path.\")\n",
    "\n",
    "# Import the feature engineering pipeline and its components\n",
    "try:\n",
    "    from src.feature_engineering import (\n",
    "        create_feature_engineering_pipeline,\n",
    "        FeatureExtractor,\n",
    "        RFMCalculator,\n",
    "        AggregateFeatures,\n",
    "        MissingValueHandler,\n",
    "        CustomEncoder,\n",
    "        FeatureScaler\n",
    "    )\n",
    "    print(\"Successfully imported feature_engineering.py components.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing feature_engineering.py: {e}\")\n",
    "    print(\"Please ensure 'src/feature_engineering.py' exists and is correctly defined.\")\n",
    "    print(\"Also, check that the project root (containing 'src') is added to sys.path.\")\n",
    "    raise # Re-raise the error to halt execution if import fails\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nSetup complete.\")\n",
    "\n",
    "DATA_PATH = '../data/raw/data.csv' # Path to your raw data file\n",
    "\n",
    "# --- Load Data ---\n",
    "print(\"--- Loading Raw Data ---\")\n",
    "try:\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Data loaded successfully from '{DATA_PATH}'\")\n",
    "    print(f\"Raw data shape: {df_raw.shape}\")\n",
    "    print(\"\\nRaw data head:\")\n",
    "    display(df_raw.head())\n",
    "    print(\"\\nRaw data info:\")\n",
    "    df_raw.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {DATA_PATH} not found.\")\n",
    "    print(\"Please ensure 'data.csv' is located in the '../data/raw/' directory relative to this notebook.\")\n",
    "\n",
    "print(\"\\n--- Testing RFMCalculator ---\")\n",
    "# Use a fresh copy of df_raw, as RFM needs the 'TransactionStartTime'\n",
    "rfm_calc = RFMCalculator()\n",
    "df_rfm = rfm_calc.fit_transform(df_raw.copy())\n",
    "print(\"After RFMCalculator (first 5 rows):\")\n",
    "display(df_rfm.head())\n",
    "print(f\"RFM columns added: {[col for col in ['Recency', 'Frequency', 'Monetary'] if col in df_rfm.columns]}\")\n",
    "print(\"\\nRFM features summary statistics:\")\n",
    "display(df_rfm[['Recency', 'Frequency', 'Monetary']].describe())\n",
    "# Use a fresh copy of df_raw for this individual test, as FeatureExtractor expects 'TransactionStartTime'\n",
    "extractor = FeatureExtractor()\n",
    "df_extracted = extractor.fit_transform(df_raw.copy())\n",
    "print(\"After FeatureExtractor (first 5 rows):\")\n",
    "display(df_extracted.head())\n",
    "print(f\"New time-based columns: {[col for col in df_extracted.columns if 'Transaction' in col and col != 'TransactionId']}\")\n",
    "print(f\"Original 'TransactionStartTime' column removed: {'TransactionStartTime' not in df_extracted.columns}\")\n",
    "# Cell 5: Test AggregateFeatures\n",
    "print(\"\\n--- Testing AggregateFeatures ---\")\n",
    "# Ensure 'Amount' is numeric before aggregation for this individual test\n",
    "df_agg_test_copy = df_raw.copy()\n",
    "df_agg_test_copy['Amount'] = pd.to_numeric(df_agg_test_copy['Amount'], errors='coerce')\n",
    "\n",
    "aggregator = AggregateFeatures()\n",
    "df_aggregated = aggregator.fit_transform(df_agg_test_copy)\n",
    "print(\"After AggregateFeatures (first 5 rows):\")\n",
    "display(df_aggregated.head())\n",
    "print(f\"Aggregate columns added: {[col for col in df_aggregated.columns if 'TransactionAmount' in col or 'TransactionCount' in col]}\")\n",
    "print(\"\\nAggregate features summary statistics:\")\n",
    "display(df_aggregated[['TotalTransactionAmount', 'AverageTransactionAmount', 'MinTransactionAmount', 'MaxTransactionAmount', 'TransactionCount', 'StdDevTransactionAmount']].describe())\n",
    "\n",
    "df_with_nans = df_raw.copy()\n",
    "# Introduce NaN in 'Amount' and 'Value' for a couple of rows\n",
    "df_with_nans.loc[[1, 5], ['Amount', 'Value']] = np.nan\n",
    "# Introduce NaN in a categorical column for testing\n",
    "df_with_nans.loc[[2, 6], 'ProductCategory'] = np.nan\n",
    "df_with_nans.loc[[3], 'ChannelId'] = np.nan\n",
    "\n",
    "print(\"Data before MissingValueHandler (showing NaNs counts):\")\n",
    "print(df_with_nans[['Amount', 'Value', 'ProductCategory', 'ChannelId']].isnull().sum())\n",
    "print(\"\\nBefore imputation - Sample rows with NaNs:\")\n",
    "display(df_with_nans[df_with_nans['Amount'].isnull() | df_with_nans['ProductCategory'].isnull()].head())\n",
    "\n",
    "imputer = MissingValueHandler(strategy='mean')\n",
    "df_imputed = imputer.fit_transform(df_with_nans.copy()) # Use a fresh copy for this test\n",
    "\n",
    "print(\"\\nData after MissingValueHandler (showing NaNs counts):\")\n",
    "print(df_imputed[['Amount', 'Value', 'ProductCategory', 'ChannelId']].isnull().sum())\n",
    "print(\"\\nAfter imputation - Sample rows (should be filled):\")\n",
    "display(df_imputed[df_imputed['TransactionId'].isin([df_with_nans.loc[1, 'TransactionId'], df_with_nans.loc[2, 'TransactionId']])])\n",
    "\n",
    "print(\"\\nValue counts for ProductCategory after imputation (should show no NaNs, most frequent used):\")\n",
    "print(df_imputed['ProductCategory'].value_counts(dropna=False))\n",
    "print(\"\\n--- Testing CustomEncoder (One-Hot Encoding) ---\")\n",
    "# Start with data that has been imputed (or a fresh copy if no NaNs expected)\n",
    "df_for_encoding = df_raw.copy()\n",
    "# Assuming MissingValueHandler might be part of the full pipeline, for individual test, handle NaNs if needed\n",
    "df_for_encoding['ProductCategory'] = df_for_encoding['ProductCategory'].fillna('Unknown')\n",
    "df_for_encoding['ChannelId'] = df_for_encoding['ChannelId'].fillna('Unknown')\n",
    "df_for_encoding['CurrencyCode'] = df_for_encoding['CurrencyCode'].fillna('Unknown')\n",
    "# Select some categorical columns for encoding\n",
    "categorical_cols_onehot = ['ProductCategory', 'ChannelId', 'CurrencyCode']\n",
    "cols_to_encode_existing = [col for col in categorical_cols_onehot if col in df_for_encoding.columns and df_for_encoding[col].dtype == 'object']\n",
    "\n",
    "encoder_onehot = CustomEncoder(method='onehot', columns=cols_to_encode_existing)\n",
    "df_encoded_onehot = encoder_onehot.fit_transform(df_for_encoding.copy())\n",
    "\n",
    "print(f\"Original categorical columns being encoded: {cols_to_encode_existing}\")\n",
    "print(f\"Shape before encoding: {df_for_encoding.shape}\")\n",
    "print(f\"Shape after One-Hot Encoding: {df_encoded_onehot.shape}\")\n",
    "print(\"\\nFirst 5 rows with new one-hot encoded columns:\")\n",
    "display(df_encoded_onehot.head())\n",
    "\n",
    "print(\"\\nPresence of original and new columns:\")\n",
    "for col in cols_to_encode_existing:\n",
    "    print(f\"Is original '{col}' column present? {col in df_encoded_onehot.columns}\")\n",
    "    example_dummy_cols = [c for c in df_encoded_onehot.columns if c.startswith(f\"{col}_\")][:3]\n",
    "    if example_dummy_cols:\n",
    "        print(f\"Example new dummy columns for '{col}': {example_dummy_cols}\")\n",
    "    else:\n",
    "        print(f\"No new dummy columns found for '{col}'. Check if it was encoded.\")\n",
    "        \n",
    "print(\"\\n--- Testing CustomEncoder (Label Encoding) ---\")\n",
    "# Use a fresh copy of the raw data for a clear demonstration of label encoding\n",
    "df_for_label_encoding = df_raw.copy()\n",
    "df_for_label_encoding['ProductCategory'] = df_for_label_encoding['ProductCategory'].fillna('Unknown') # Handle NaNs for label encoding\n",
    "\n",
    "categorical_cols_label = ['ProductId', 'ProductCategory'] # Example columns for label encoding\n",
    "\n",
    "encoder_label = CustomEncoder(method='label', columns=categorical_cols_label)\n",
    "df_encoded_label = encoder_label.fit_transform(df_for_label_encoding.copy())\n",
    "\n",
    "print(f\"Original categorical columns being encoded: {categorical_cols_label}\")\n",
    "print(\"\\nFirst 5 rows with label encoded columns:\")\n",
    "display(df_encoded_label[categorical_cols_label].head())\n",
    "\n",
    "print(\"\\nUnique encoded values for 'ProductId':\")\n",
    "print(df_encoded_label['ProductId'].unique())\n",
    "print(\"\\nUnique encoded values for 'ProductCategory':\")\n",
    "print(df_encoded_label['ProductCategory'].unique())\n",
    "print(f\"Original 'ProductId' dtype: {df_for_label_encoding['ProductId'].dtype}, Encoded 'ProductId' dtype: {df_encoded_label['ProductId'].dtype}\")\n",
    "print(\"\\n--- Testing FeatureScaler (StandardScaler) ---\")\n",
    "# To test scaler, we need numerical data. Let's create a derived dataframe for this test.\n",
    "# Step 1: Apply RFM\n",
    "temp_df_for_scaling = RFMCalculator().fit_transform(df_raw.copy())\n",
    "# Step 2: Apply FeatureExtractor (which drops TransactionStartTime)\n",
    "temp_df_for_scaling = FeatureExtractor().fit_transform(temp_df_for_scaling)\n",
    "# Step 3: Apply AggregateFeatures\n",
    "temp_df_for_scaling = AggregateFeatures().fit_transform(temp_df_for_scaling)\n",
    "# Step 4: Handle Missing Values (important before scaling)\n",
    "# Determine numerical and categorical columns dynamically\n",
    "numerical_cols = temp_df_for_scaling.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = temp_df_for_scaling.select_dtypes(include='object').columns.tolist()\n",
    "imputer_for_scaler_test = MissingValueHandler(numerical_cols=numerical_cols, categorical_cols=categorical_cols)\n",
    "temp_df_for_scaling = imputer_for_scaler_test.fit_transform(temp_df_for_scaling)\n",
    "\n",
    "# Identify numerical columns to scale from the prepared temporary dataframe\n",
    "numerical_cols_to_scale = temp_df_for_scaling.select_dtypes(include=np.number).columns.tolist()\n",
    "# Exclude identifier columns that shouldn't be scaled (if they are numeric)\n",
    "# These IDs might not be present if removed by other transformers, but good to list\n",
    "ids_to_exclude = ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId',\n",
    "                  'CountryCode', 'PricingStrategy', 'FraudResult', 'ProviderId']\n",
    "numerical_cols_to_scale = [col for col in numerical_cols_to_scale if col not in ids_to_exclude]\n",
    "\n",
    "print(f\"Numerical columns selected for scaling: {numerical_cols_to_scale}\")\n",
    "\n",
    "if not numerical_cols_to_scale:\n",
    "    print(\"No suitable numerical columns found for scaling after previous steps. Skipping scaler test.\")\n",
    "else:\n",
    "    scaler = FeatureScaler(method='standard', columns=numerical_cols_to_scale)\n",
    "    df_scaled = scaler.fit_transform(temp_df_for_scaling.copy())\n",
    "\n",
    "    print(\"\\nAfter FeatureScaler (first 5 rows of scaled numerical columns):\")\n",
    "    display(df_scaled[numerical_cols_to_scale].head())\n",
    "\n",
    "    print(\"\\nMean and Standard Deviation of scaled columns (should be ~0 and ~1):\")\n",
    "    display(df_scaled[numerical_cols_to_scale].describe().loc[['mean', 'std']])\n",
    "print(\"\\n--- Testing Full Feature Engineering Pipeline ---\")\n",
    "\n",
    "# It's good practice to ensure df_raw is loaded just before calling the full pipeline,\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "# Initialize the full pipeline\n",
    "full_pipeline = create_feature_engineering_pipeline(\n",
    "    numerical_imputation_strategy='mean',\n",
    "    categorical_encoding_method='onehot'\n",
    ")\n",
    "# Apply the full pipeline to the raw data\n",
    "print(f\"Shape of raw data: {df_raw.shape}\")\n",
    "df_processed_full = full_pipeline.fit_transform(df_raw.copy()) # Use a fresh copy of the raw data\n",
    "\n",
    "print(\"\\n--- Full Processed Data Overview ---\")\n",
    "print(f\"Shape of fully processed data: {df_processed_full.shape}\")\n",
    "print(\"\\nFirst 5 rows of fully processed data:\")\n",
    "display(df_processed_full.head())\n",
    "print(\"\\nInformation about fully processed columns:\")\n",
    "df_processed_full.info()\n",
    "\n",
    "print(\"\\n--- Final Check for Missing Values in Fully Processed Data ---\")\n",
    "final_missing_values = df_processed_full.isnull().sum()\n",
    "final_missing_values = final_missing_values[final_missing_values > 0]\n",
    "print(final_missing_values)\n",
    "if final_missing_values.empty:\n",
    "    print(\"No missing values found in the final processed data.\")\n",
    "else:\n",
    "    print(\"Warning: Missing values still present after full pipeline. Investigate columns listed above.\")\n",
    "\n",
    "print(\"\\n--- Feature Engineering Pipeline Testing Completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd4f28-d486-4e21-b3da-e38c97ebe46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

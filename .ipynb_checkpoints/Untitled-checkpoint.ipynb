{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ef63fa-967e-4832-950f-e10e4801c131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'D:\\10academy\\week5' to sys.path.\n",
      "All necessary libraries and custom modules imported.\n",
      "Error: data.csv not found at D:\\10academy\\week5\\data\\raw\\data.csv. Please ensure the data file is in the correct location: D:\\10academy\\week5\\data\\raw\\data.csv\n",
      "Skipping Task 4: Raw data not loaded.\n",
      "Skipping Task 4 K-Means: Raw data not loaded.\n",
      "Skipping Task 4 High-Risk Label: Raw data not loaded.\n",
      "Skipping Task 4 Integration: Raw data not loaded.\n",
      "Skipping Task 5: Raw data not loaded.\n",
      "Skipping Task 5 Model Training: Raw data not loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Initial Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Assuming the notebook is in 'your_project/notebooks/'\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Added '{project_root}' to sys.path.\")\n",
    "from src.feature_engineering import (\n",
    "    create_feature_engineering_pipeline,\n",
    "    RFMCalculator, # Needed for direct RFM calculation for clustering\n",
    "    FeatureExtractor,\n",
    "    AggregateFeatures,\n",
    "    CustomEncoder,\n",
    "    MissingValueHandler,\n",
    "    FeatureScaler,\n",
    "    clean_column_names \n",
    ")\n",
    "\n",
    "# Import modeling libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "print(\"All necessary libraries and custom modules imported.\")\n",
    "\n",
    "DATA_PATH = os.path.join(project_root, 'data', 'raw', 'data.csv')\n",
    "\n",
    "# Load the raw data\n",
    "try:\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Raw data loaded successfully. Shape: {df_raw.shape}\")\n",
    "    print(\"\\nRaw data head:\")\n",
    "    display(df_raw.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: data.csv not found at {DATA_PATH}. Please ensure the data file is in the correct location: {DATA_PATH}\")\n",
    "    df_raw = None # Set to None to prevent further errors if data is not loaded \n",
    "\n",
    "if df_raw is None:\n",
    "    print(\"Skipping Task 4: Raw data not loaded.\")\n",
    "else:\n",
    "    print(\"\\n--- Task 4: Proxy Target Variable Engineering ---\")\n",
    "    print(\"\\n--- Preparing Data for RFM Clustering ---\")\n",
    "\n",
    "    rfm_pipeline_step = Pipeline([\n",
    "        ('rfm_calculator', RFMCalculator())\n",
    "    ])\n",
    "\n",
    "    df_with_rfm = rfm_pipeline_step.fit_transform(df_raw.copy())\n",
    "\n",
    "    customer_rfm_df = df_with_rfm.groupby('AccountId').agg(\n",
    "        Recency=('Recency', 'first'),\n",
    "        Frequency=('Frequency', 'first'),\n",
    "        Monetary=('Monetary', 'first')\n",
    "    ).reset_index()\n",
    "\n",
    "    customer_rfm_df.fillna(\n",
    "        {'Recency': customer_rfm_df['Recency'].max() + 1 if not customer_rfm_df['Recency'].empty else 0,\n",
    "         'Frequency': 0,\n",
    "         'Monetary': 0},\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    print(f\"Customer RFM data shape: {customer_rfm_df.shape}\")\n",
    "    print(\"\\nCustomer RFM data head:\")\n",
    "    display(customer_rfm_df.head())\n",
    "    print(\"\\nCustomer RFM data info:\")\n",
    "    customer_rfm_df.info()\n",
    "\n",
    "    # Store AccountId for later merging\n",
    "    customer_ids = customer_rfm_df[['AccountId']].copy()\n",
    "\n",
    "    # Select only the RFM features for clustering\n",
    "    rfm_features = customer_rfm_df[['Recency', 'Frequency', 'Monetary']]\n",
    "\n",
    "    print(\"\\nRFM features for clustering (first 5 rows):\")\n",
    "    display(rfm_features.head())\n",
    "\n",
    "if df_raw is None:\n",
    "    print(\"Skipping Task 4 K-Means: Raw data not loaded.\")\n",
    "else:\n",
    "    print(\"\\n--- Scaling RFM Features ---\")\n",
    "    scaler = StandardScaler()\n",
    "    rfm_scaled = scaler.fit_transform(rfm_features)\n",
    "    rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=rfm_features.columns, index=rfm_features.index)\n",
    "\n",
    "    print(\"\\nScaled RFM features (first 5 rows):\")\n",
    "    display(rfm_scaled_df.head())\n",
    "\n",
    "\n",
    "    print(\"\\n--- Performing K-Means Clustering (K=3) ---\")\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10) # n_init to suppress warning\n",
    "    customer_rfm_df['Cluster'] = kmeans.fit_predict(rfm_scaled_df)\n",
    "\n",
    "    print(\"\\nCustomer RFM data with cluster assignments (first 5 rows):\")\n",
    "    display(customer_rfm_df.head())\n",
    "\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    print(customer_rfm_df['Cluster'].value_counts().sort_index())\n",
    "\n",
    "    print(\"\\nCluster centroids (after scaling):\")\n",
    "    cluster_centroids_scaled = pd.DataFrame(kmeans.cluster_centers_, columns=rfm_features.columns)\n",
    "    display(cluster_centroids_scaled)\n",
    "\n",
    "    print(\"\\nCluster centroids (original scale - inverse transformed):\")\n",
    "    cluster_centroids_original_scale = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=rfm_features.columns)\n",
    "    display(cluster_centroids_original_scale)\n",
    "\n",
    "    # Visualize clusters (optional, for analysis)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x='Recency', y='Frequency', hue='Cluster', data=customer_rfm_df, palette='viridis', s=100, alpha=0.7)\n",
    "    plt.title('Customer Clusters based on Recency and Frequency')\n",
    "    plt.xlabel('Recency (Days)')\n",
    "    plt.ylabel('Frequency (Transactions)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x='Monetary', y='Frequency', hue='Cluster', data=customer_rfm_df, palette='viridis', s=100, alpha=0.7)\n",
    "    plt.title('Customer Clusters based on Monetary and Frequency')\n",
    "    plt.xlabel('Monetary Value')\n",
    "    plt.ylabel('Frequency (Transactions)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "if df_raw is None:\n",
    "    print(\"Skipping Task 4 High-Risk Label: Raw data not loaded.\")\n",
    "else:\n",
    "    print(\"\\n--- Defining and Assigning 'High-Risk' Label ---\")\n",
    "    high_risk_cluster_id = 0 \n",
    "\n",
    "    customer_rfm_df['is_high_risk'] = customer_rfm_df['Cluster'].apply(\n",
    "        lambda x: 1 if x == high_risk_cluster_id else 0\n",
    "    )\n",
    "\n",
    "    print(f\"Assigned high-risk label (1) to Cluster {high_risk_cluster_id}.\")\n",
    "    print(\"\\nDistribution of 'is_high_risk' label:\")\n",
    "    print(customer_rfm_df['is_high_risk'].value_counts())\n",
    "\n",
    "    print(\"\\nSample of customer_rfm_df with 'is_high_risk' label:\")\n",
    "    display(customer_rfm_df.sample(5))\n",
    "# Cell 6: Integrate the Target Variable into the Main Dataset\n",
    "\n",
    "if df_raw is None:\n",
    "    print(\"Skipping Task 4 Integration: Raw data not loaded.\")\n",
    "else:\n",
    "    print(\"\\n--- Integrating Target Variable into Main Processed Dataset ---\")\n",
    "\n",
    "    full_pipeline = create_feature_engineering_pipeline(\n",
    "        numerical_imputation_strategy='mean',\n",
    "        categorical_encoding_method='onehot'\n",
    "    )\n",
    "\n",
    "    df_processed_main = full_pipeline.fit_transform(df_raw.copy())\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Shape of fully processed data before merging target: {df_processed_main.shape}\")\n",
    "    print(\"\\nProcessed data columns before merge:\")\n",
    "    print(df_processed_main.columns.tolist())\n",
    "\n",
    "    df_processed_final = pd.merge(\n",
    "        df_processed_main,\n",
    "        customer_rfm_df[['AccountId', 'is_high_risk']],\n",
    "        on='AccountId',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    df_processed_final['is_high_risk'] = df_processed_final['is_high_risk'].fillna(0).astype(int)\n",
    "\n",
    "    print(f\"Shape of final processed data with target variable: {df_processed_final.shape}\")\n",
    "    print(\"\\nFinal processed data with 'is_high_risk' (first 5 rows):\")\n",
    "    display(df_processed_final.head())\n",
    "    print(\"\\nDistribution of final 'is_high_risk' label in the full dataset:\")\n",
    "    print(df_processed_final['is_high_risk'].value_counts())\n",
    "    print(\"\\nInfo of final processed data:\")\n",
    "    df_processed_final.info()\n",
    "\n",
    "    print(\"\\n--- Task 4 Completed ---\")\n",
    "\n",
    "if df_raw is None:\n",
    "    print(\"Skipping Task 5: Raw data not loaded.\")\n",
    "else:\n",
    "    print(\"\\n--- Task 5: Model Training and Tracking ---\")\n",
    "\n",
    "    # --- MLflow Setup ---\n",
    "    mlflow.set_tracking_uri(\"file:///./mlruns\") # Logs to a local 'mlruns' directory\n",
    "    mlflow.set_experiment(\"Credit_Risk_Scoring_Model\")\n",
    "    print(f\"MLflow tracking URI set to: {mlflow.get_tracking_uri()}\")\n",
    "    print(f\"MLflow experiment set to: {mlflow.get_experiment_by_name('Credit_Risk_Scoring_Model').experiment_id}\")\n",
    "\n",
    "    columns_to_drop = [\n",
    "        'TransactionId', 'BatchId', 'SubscriptionId', 'CustomerId', # Identifiers\n",
    "        'AccountId'\n",
    "    ]\n",
    "\n",
    "    # Filter out columns that might not exist in df_processed_final'\n",
    "    features_df = df_processed_final.drop(\n",
    "        columns=[col for col in columns_to_drop if col in df_processed_final.columns and col != 'is_high_risk'],\n",
    "        errors='ignore'\n",
    "    )\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = features_df.drop('is_high_risk', axis=1)\n",
    "    y = features_df['is_high_risk']\n",
    "\n",
    "    print(f\"\\nFeatures (X) shape: {X.shape}\")\n",
    "    print(f\"Target (y) shape: {y.shape}\")\n",
    "    print(\"\\nFeatures (X) head:\")\n",
    "    display(X.head())\n",
    "    print(\"\\nTarget (y) value counts:\")\n",
    "    print(y.value_counts())\n",
    "\n",
    "\n",
    "    # --- Split Data ---\n",
    "    print(\"\\n--- Splitting Data into Training and Testing Sets ---\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train value counts:\\n{y_train.value_counts(normalize=True)}\")\n",
    "    print(f\"y_test value counts:\\n{y_test.value_counts(normalize=True)}\")\n",
    "\n",
    "    print(\"\\n--- Model Training Setup Complete ---\")\n",
    "# Cell 8: Model Training, Hyperparameter Tuning, and Evaluation with MLflow\n",
    "\n",
    "if df_raw is None:\n",
    "    print(\"Skipping Task 5 Model Training: Raw data not loaded.\")\n",
    "else:\n",
    "    print(\"\\n--- Training, Tuning, and Evaluating Models ---\")\n",
    "\n",
    "    # Define models and their hyperparameter grids for GridSearchCV\n",
    "    models = {\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(solver='liblinear', random_state=42),\n",
    "            'params': {\n",
    "                'C': [0.1, 1.0, 10.0],\n",
    "                'penalty': ['l1', 'l2']\n",
    "            }\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100],\n",
    "                'learning_rate': [0.05, 0.1, 0.2],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "    best_roc_auc = -1\n",
    "\n",
    "    # Iterate through models, perform GridSearchCV, log with MLflow\n",
    "    for model_name, config in models.items():\n",
    "      with mlflow.start_run(run_name=f\"{model_name}_GridSearch\"):\n",
    "        print(f\"\\n--- Training {model_name} ---\")\n",
    "\n",
    "        classifier = config['model']\n",
    "        param_grid = config['params']\n",
    "\n",
    "        # Log model parameters (the full search grid)\n",
    "        mlflow.log_params({f\"param_{k}\": str(v) for k, v in param_grid.items()})\n",
    "\n",
    "        print(f\"\\n--- Debugging {model_name} inputs ---\")\n",
    "        \n",
    "         \n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "        # Define columns to drop for consistency\n",
    "        problematic_cols = ['ProviderId', 'ProductId']\n",
    "\n",
    "        # Apply the temporary fix to X_train\n",
    "        X_train_processed = X_train.drop(columns=[col for col in problematic_cols if col in X_train.columns], errors='ignore').copy()\n",
    "\n",
    "        # Apply the SAME temporary fix to X_test\n",
    "        X_test_processed = X_test.drop(columns=[col for col in problematic_cols if col in X_test.columns], errors='ignore').copy()\n",
    "\n",
    "        current_X_train_for_fit = X_train_processed\n",
    "        current_X_test_for_predict = X_test_processed # For prediction later in this loop\n",
    "\n",
    "        # Check data types in X_train_processed (used for fit)\n",
    "        non_numeric_cols = current_X_train_for_fit.select_dtypes(exclude=np.number).columns\n",
    "        if not non_numeric_cols.empty:\n",
    "            print(f\"!!! Warning: Non-numeric columns found in current_X_train_for_fit: {non_numeric_cols.tolist()}\")\n",
    "            for col in non_numeric_cols:\n",
    "                print(f\"Column '{col}' dtypes: {current_X_train_for_fit[col].apply(type).value_counts()}\")\n",
    "                sample_values = current_X_train_for_fit[col][current_X_train_for_fit[col].apply(lambda x: not isinstance(x, (int, float, np.number)))].unique()\n",
    "                print(f\"  Sample non-numeric values in '{col}': {sample_values[:10]}\")\n",
    "        else:\n",
    "            print(\"All columns in current_X_train_for_fit are numeric. Good.\")\n",
    "\n",
    "        # Ensure all columns are numeric in current_X_train_for_fit\n",
    "        for col in current_X_train_for_fit.columns:\n",
    "            try:\n",
    "                current_X_train_for_fit[col] = pd.to_numeric(current_X_train_for_fit[col], errors='raise')\n",
    "            except ValueError as e:\n",
    "                print(f\"Error converting column '{col}' in current_X_train_for_fit to numeric: {e}\")\n",
    "                print(f\"  Sample values in problematic column: {current_X_train_for_fit[col].head()}\")\n",
    "                raise\n",
    "\n",
    "        # Also ensure X_test_processed is numeric before prediction\n",
    "        for col in current_X_test_for_predict.columns:\n",
    "            try:\n",
    "                current_X_test_for_predict[col] = pd.to_numeric(current_X_test_for_predict[col], errors='raise')\n",
    "            except ValueError as e:\n",
    "                print(f\"Error converting column '{col}' in current_X_test_for_predict to numeric: {e}\")\n",
    "                print(f\"  Sample values in problematic column: {current_X_test_for_predict[col].head()}\")\n",
    "                raise\n",
    "\n",
    "        print(\"X_train and X_test successfully processed for current run.\")\n",
    "\n",
    "\n",
    "        grid_search = GridSearchCV(classifier, param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "        # Use the processed X_train for fitting\n",
    "        grid_search.fit(current_X_train_for_fit, y_train)\n",
    "\n",
    "        # Get the best model from Grid Search\n",
    "        best_clf = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "\n",
    "        print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "        print(f\"Best cross-validation ROC-AUC for {model_name}: {best_score:.4f}\")\n",
    "\n",
    "        # Log best parameters and best CV score\n",
    "        mlflow.log_params({f\"best_param_{k}\": v for k, v in best_params.items()})\n",
    "        mlflow.log_metric(\"best_cv_roc_auc\", best_score)\n",
    "\n",
    "        # Evaluate the best model on the test set - USE THE PROCESSED X_test\n",
    "        y_pred = best_clf.predict(current_X_test_for_predict)\n",
    "        y_proba = best_clf.predict_proba(current_X_test_for_predict)[:, 1] # Probability of the positive class\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Test Precision: {precision:.4f}\")\n",
    "        print(f\"Test Recall: {recall:.4f}\")\n",
    "        print(f\"Test F1 Score: {f1:.4f}\")\n",
    "        print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        # Log test metrics\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"test_precision\", precision)\n",
    "        mlflow.log_metric(\"test_recall\", recall)\n",
    "        mlflow.log_metric(\"test_f1_score\", f1)\n",
    "        mlflow.log_metric(\"test_roc_auc\", roc_auc)\n",
    "\n",
    "        # Log the model (ensure unique name if not registering to avoid conflicts)\n",
    "        mlflow.sklearn.log_model(best_clf, \"model\", signature=mlflow.models.infer_signature(X_train, y_pred))\n",
    "\n",
    "\n",
    "        # Track the best model overall for later registration\n",
    "        if roc_auc > best_roc_auc:\n",
    "            best_roc_auc = roc_auc\n",
    "            best_model = best_clf\n",
    "            best_model_name = model_name\n",
    "\n",
    "    print(f\"\\n--- Best Model Identified: {best_model_name} with ROC-AUC: {best_roc_auc:.4f} ---\")\n",
    "\n",
    "    # Register the overall best model in MLflow Model Registry\n",
    "    if best_model is not None:\n",
    "        with mlflow.start_run(run_name=f\"Register_Best_Model_{best_model_name}\"):\n",
    "            # Ensure the registered model name is consistent\n",
    "            registered_model_name = \"CreditRisk_HighRisk_Model\"\n",
    "            mlflow.sklearn.log_model(\n",
    "                best_model,\n",
    "                \"best_model_artifact\", # Artifact path within the run\n",
    "                registered_model_name=registered_model_name,\n",
    "                signature=mlflow.models.infer_signature(X_train, best_model.predict(X_train)),\n",
    "                tags={\"model_type\": best_model_name, \"task\": \"high_risk_prediction\", \"roc_auc\": f\"{best_roc_auc:.4f}\"}\n",
    "            )\n",
    "            print(f\"Registered best model '{best_model_name}' as '{registered_model_name}' in MLflow Model Registry.\")\n",
    "\n",
    "    print(\"\\n--- Model Training and Evaluation Completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c71fb-a99c-4746-bfd6-c6d9d1f963bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
